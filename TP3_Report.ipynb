{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ROB 311 - TP 3\n",
    "## Q-Learning\n",
    "### David VELASQUEZ OSPINA\n",
    "### Ricardo RICO URIBE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Coded Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def __init__(self, **args):\n",
    "        ReinforcementAgent.__init__(self, **args)\n",
    "\n",
    "        self.qTable = util.Counter()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "We initialise the Q table as a counter, a modified dictionary with added capabilities. This is important as we need the values to be not null from undetermined keys until they are assigned.\n",
    "\n",
    "This table assigs a pair of state and action a \"Q\" value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def getQValue(self, state, action):\n",
    "        return self.qTable[(state, action)]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Returns the Q value saved with the keys state and action, this is similar to a value in a 2 coordinate system."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeValueFromQValues(self, state):\n",
    "    legalActions = self.getLegalActions(state)\n",
    "    semiQTable = util.Counter()\n",
    "\n",
    "    if len(legalActions) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        for action in legalActions:\n",
    "        semiQTable[action] = self.getQValue(state,action)\n",
    "    return semiQTable[semiQTable.argMax()]"
   ]
  },
  {
   "source": [
    "We want to return the value corresponging to the optimal policy of a state, this means to return the max value between all the values for each action in an specified state.\n",
    "\n",
    "If the state has no legal actions, we return 0, if there are legal actions we save them with their corresponging value in a new dictionary, and then we return the biggest value saved in it. We use the function \"argMax()\" from the class \"Counter\" which returns the key with the maximal value, but as we need the value not the key, we use the key as the search term of the dictionary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeActionFromQValues(self, state):\n",
    "    legalActions = self.getLegalActions(state)\n",
    "    semiQTable = util.Counter()\n",
    "\n",
    "    if len(legalActions) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        for action in legalActions:\n",
    "        semiQTable[action] = self.getQValue(state,action)\n",
    "    return semiQTable.argMax()"
   ]
  },
  {
   "source": [
    "We first save all legal actions, if there are none we return this. otherwise we save all posible actions with their q value and with the use of the \"argMax()\" function we return the action with the bigger Q value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(self, state):\n",
    "    legalActions = self.getLegalActions(state)\n",
    "    action = None\n",
    "    \n",
    "    if len(legalActions) != 0:\n",
    "        if util.flipCoin(self.epsilon):\n",
    "        action = random.choice(legalActions)\n",
    "        else:\n",
    "        action = self.computeActionFromQValues(state)\n",
    "    return action"
   ]
  },
  {
   "source": [
    "To change between exploration and exploitation we use the value epsilon to determine if we are going to read from the Q table (exploitation) or to perform a random action (exploration). When exploiting the action performed will be the optimal one.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, state, action, nextState, reward):\n",
    "    oldQ = self.getQValue(state, action)\n",
    "    nextStateQ = self.computeValueFromQValues(nextState)\n",
    "    self.qTable[state, action] = (1 - self.alpha)*oldQ + self.alpha*(reward + self.discount*nextStateQ)"
   ]
  },
  {
   "source": [
    "To update the Q table we use the Q-learning funcion (seen below) based of the optimal value function. This function takes into account the state which will be reached when the action is performed, as well as the learning rate and the discount rate, alpha or learning rate indicates how much the Q table will be overwritten rather than saving the same value. The discount rate decides how much of the future state value will affect the current state.\n",
    "\n",
    "The Q table is updated constantly during the learning face (while there are uncompleted learning episodes) after this, there are no more updates an the agent only reads the table to perform the optimal action.\n",
    "\n",
    "![alt text](./images/Q_formula.jpeg \"Q(s,a) Formula\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "At the beggining of the training the agent doesnt knows the environment and doesnt have the table to make optimal moves, so it just performs random actions. Resulting in terrible moves and low scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}